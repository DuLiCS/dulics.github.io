---

layout: post
title: MediaPipe Hands: On-device Real-time Hand Tracking
subtitle: Paper Reading
tags: [Paper Reading, Machine Learning]

---


<head>
    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
            inlineMath: [['$','$']]
            }
        });
    </script>
</head>




## MediaPipe Hands: On-device Real-time Hand Tracking



è¿™ç¯‡è®ºæ–‡ç¬”è®°æ˜¯å¯¹æ–‡ç« MediaPipe Hands: On-device Real-time Hand Trackingçš„é˜…è¯»ç¬”è®°ã€‚

é˜…è¯»ç­‰çº§ï¼š

*  æ³›è¯»

åˆ†ç±»ï¼š

*  æ„Ÿå…´è¶£æ–‡ç« 


### Abstract

We present **a real-time on-device hand tracking solution** that predicts a hand skeleton of a human from a single RGB camera for AR/VR applications.

Our pipeline consists of two models: **1) a palm detector**, that is providing a bounding box of a hand to, 2) **a hand landmark model**, that is predicting the hand skeleton.

### Intro

Our main contributions are:

An efficient two-stage hand tracking pipeline that can track multiple hands in real-time on mobile devices.
A hand pose estimation model that is capable of pre- dicting 2.5D hand pose with only RGB input.
And open source hand tracking pipeline as a ready-to-go solution on a variety of platforms, including An- droid, iOS, Web and desktop PCs.

### Architecture

Our hand tracking solution utilizes an ML pipeline consisting of two models working together:
1


Apalmdetectorthatoperatesonafullinputimageand locates palms via an oriented hand bounding box.
A hand landmark model that operates on the cropped hand bounding box provided by the palm detector and returns high-fidelity 2.5D landmarks.


### Datasets

In-the-wild dataset: This dataset contains 6K images of large variety, e.g. geographical diversity, various lighting conditions and hand appearance. The limita- tion of this dataset is that it doesnâ€™t contain complex articulation of hands.
In-house collected gesture dataset: This dataset con- tains 10K images that cover various angles of all phys- ically possible hand gestures. The limitation of this dataset is that itâ€™s collected from only 30 people with limited variation in background. The in-the-wild and in-house dataset are great complements to each other to improve robustness.
Synthetic dataset: To even better cover the possi- ble hand poses and provide additional supervision for depth, we render a high-quality synthetic hand model over various backgrounds and map it to the corre- sponding 3D coordinates. We use a commercial 3D hand model that is rigged with 24 bones and includes 36 blendshapes, which control fingers and palm thick- ness. The model also provides 5 textures with differ- ent skin tones. We created video sequences of trans- formation between hand poses and sampled 100K im- ages from the videos. We rendered each pose with a random high-dynamic-range lighting environment and three different cameras. See Figure 4 for examples.


### Implementation

* One key optimization MediaPipe provides is that the palm detector only runs as needed (fairly infrequently), saving significant computation. 

### Conclusion

In this paper, we proposed MediaPipe Hands, an end-to- end hand tracking solution that achieves real-time perfor- mance on multiple platforms. Our pipeline predicts 2.5D landmarks without any specialized hardware and thus, can be easily deployed to commodity devices. We open sourced the pipeline to encourage researchers and engineers to build gesture control and creative AR/VR applications with our pipeline.



### è¯»åæ„Ÿ


é€šç¯‡æ–‡ç« å¾ˆç®€å•ï¼Œæ²¡æœ‰æ•°å­¦å…¬å¼ã€‚ä¼˜è¶Šæ€§åœ¨äºå®æ—¶ï¼ŒåŒ…å«äº†å‡ ä¸ªéƒ¨åˆ†ï¼Œæ‰‹æŒæ£€æµ‹å’Œå…³é”®ç‚¹ã€‚æ‰‹æŒæ£€æµ‹åªåœ¨å¼€å§‹å’Œæ²¡æœ‰æ£€æµ‹åˆ°æ‰‹çš„æ—¶å€™å¼€å¯ã€‚
æ„å»ºäº†æ•°æ®é›†ï¼Œä½¿ç”¨äº†ç¥ç»ç½‘ç»œä½œä¸ºdecoderã€‚

ä¸»è¦æ˜¯ä½¿ç”¨ï¼Œæ–‡ç« çœ‹äº†ä¸‹æ²¡æœ‰ä»€ä¹ˆï¼Œä¸»è¦æ˜¯å—å¯ä»¥å®æ—¶ï¼Œå†å°±æ˜¯å¤šå¹³å°åŒ…æ‹¬ç§»åŠ¨è®¾å¤‡é€šç”¨ã€‚