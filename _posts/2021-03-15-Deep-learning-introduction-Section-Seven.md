---

layout: post
title: 深度学习入门  基于Python的理论实现
subtitle: 第七章  卷积神经网络
tags: [Machine learning, Reading]

---

<head>
    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
            inlineMath: [['$','$']]
            }
        });
    </script>
</head>



# 第七章 卷积神经网络

终于来到了激动人心的卷积神经网络。卷积神经网络的重要性不用多说，所以这一章着重来介绍。

### 7.1 整体结构

首先看一下CNN的网络结构。神经网络和之前的神经网络一样，可以像乐高积木一样通过组装层来构建，不过出现了新的卷积层（Convolution层）和池化层（Pooling层）。

之前介绍的神经网络中，相邻的所有神经元只见痘有链接，称为**全连接**。结构类似下图所示。

![CNN1.png](/img/CNN1.png)

对比一下，CNN的结构是什么样的。下图就是一个CNN的例子。

![CNN2.png](/img/CNN2.png)

经过对比可以看出，多了卷积层和池化层。其连接顺序是“Convolution - ReLU - （Pooling）”（Pooling层有时也会被省略）。

### 7.2 卷积层

CNN 中出现了一些特有的术语，比如填充、步幅等。此外，各层中传 递的数据是有形状的数据(比如，3 维数据)，这与之前的全连接网络不同， 因此刚开始学习 CNN 时可能会感到难以理解。本节我们将花点时间，认真 学习一下 CNN 中使用的卷积层的结构。


#### 7.2.1 全连接层存在的问题

之前介绍的全连接的神经网络中使用了全连接层(Affine 层)。在全连接 层中，相邻层的神经元全部连接在一起，输出的数量可以任意决定。

全连接层存在什么问题呢?那就是数据的形状被“忽视”了。比如，输入数据是图像时，图像通常是高、长、通道方向上的3维形状。但是，向全连接层输入时，需要将3维数据拉平为1维数据。实际上，前面提到的使用了MNIST数据集的例子中，输入图像就是1通道、高28像素、长28像素的(1, 28, 28)形状，但却被排成1列，以784个数据的形式输入到最开始的 Affine 层。

其实就是说，作为图像来说，三维图像本身就蕴含一些信息，全连接层把图像拉长，使得一些信息被去掉。

卷积层可以保持形状不变，输入是图像时，卷积层会以三维数据的形式接收输入数据，并用三维形式输出到下一层。因此，**在 CNN 中，可以(有可能)正确理解图像等具有形状的数据。**

CNN 中，有时将卷积层的输入输出数据称为**特征图**(feature map)。其中，卷积层的输入数据称为**输入特征图**(input feature map)，输出 数据称为输出特征图(output feature map)。本书中将“输入输出数据”和“特
征图”作为含义相同的词使用。

#### 7.2.2 卷积运算

卷积层进行的处理就是卷积运算。卷积运算相当于图像处理中的“滤波 器运算”。

![CNN3.png](/img/CNN3.png)

上面吧滤波器在输入数据上移动，对应位置相乘最后整个相加。

![CNN4.png](/img/CNN4.png)

下面是加上某个固定值的图。

![CNN5.png](/img/CNN5.png)


#### 7.2.3 填充


在进行卷积层的处理之前，有时要向输入数据的周围填入固定的数据(比 如 0 等)，这称为填充(padding)，是卷积运算中经常会用到的处理。下面是填充0的情况。

![CNN6.png](/img/CNN6.png)


使用填充主要是为了调整输出的大小。比如，对大小为 (4, 4) 的输入 数据应用 (3, 3) 的滤波器时，输出大小变为 (2, 2)，相当于输出大小 比输入大小缩小了 2 个元素。这在反复进行多次卷积运算的深度网 络中会成为问题。为什么呢?因为如果每次进行卷积运算都会缩小 空间，那么在某个时刻输出大小就有可能变为 1，导致无法再应用 卷积运算。为了避免出现这样的情况，就要使用填充。在刚才的例 子中，将填充的幅度设为 1，那么相对于输入大小 (4, 4)，输出大小 也保持为原来的 (4, 4)。因此，卷积运算就可以在保持空间大小不变 的情况下将数据传给下一层。

#### 7.2.4 步幅

应用滤波器的位置间隔称为步幅(stride)。之前的例子中步幅都是1，如
果将步幅设为2，则如图所示，应用滤波器的窗口的间隔变为2个元素。

![CNN7.png](/img/CNN7.png)

直接总结式子。这里，假设输入大小为 $(H, W)$，滤波器大小为 $(FH, FW)$，输出大小为 $(OH, OW)$，填充为 $P$，步幅为 $S$。此时，输出大小可用如下式子进行计算。

$$
OH = \frac{H + 2 P - FH}{S} + 1
$$


$$
OW = \frac{W + 2 P - FW}{S} + 1 
$$

#### 7.2.5 3维数据的卷积运算

之前的卷积运算的例子都是以有高、长方向的 2 维形状为对象的。但是， 图像是 3 维数据，除了高、长方向之外，还需要处理通道方向。这里，我们按 照与之前相同的顺序，看一下对加上了通道方向的 3 维数据进行卷积运算的例子。
